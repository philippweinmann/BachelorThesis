%% LaTeX2e class for student theses
%% sections/abstract_en.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.5, 2020-06-26

\Abstract
In this bachelor thesis we analyse classification results using a 2017 published method called shap \cite{shapPaper}. Explaining how an artificial neural network makes a decision is an interdisciplinary research subject combining computer science, mathematics and psychology. We have studied these explanations from a psychological standpoint and after presenting our findings, we will propose a method to improve the interpretability of text explanations using text-hierarchies, without losing much/any accuracy. In addition, the goal was to test out a framework developed to analyse a multitude of explanation methods. This Framework will be presented next to our findings and how to use it to create your own analysis. This Bachelor thesis is addressed at people familiar with artificial neural networks and other machine learning methods. Background knowledge about artificial intelligence explainers is not required.
